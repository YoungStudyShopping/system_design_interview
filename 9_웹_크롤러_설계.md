# 9. 웹 크롤러 설계

- 크롤러는 로봇 또는 스파이더라 불림
- 검색엔진에서 널리 쓰이는 기술로, 신규 / 갱신된 컨텐츠를 찾는 것이 주된 목적
- 다양한 이용 사례
  - 검색 엔진 인덱싱: 구글 등 가장 보편적인 용례
  - 웹 아카이빙: 도서관 등에서 사용
  - 웹 마이닝: 주주 총회 자료 등을 받아 분석하는 등
  - 웹 모니터링: 디지마크 등에서 크롤링해서 저작권 침해 찾음

## 1. 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘
  1. url 집합이 입력으로 주어지면, 해당 url 이 가리키는 모든 웹 페이지 다운로드
  2. 다운받은 웹 페이지에서 url 추출
  3. 추출된 url 을 다운로드 목록에 추가 후, 1번으로 돌아가 반복
- 설계 진행 전 요구사항 질문
  - Q: 크롤러의 용도는?
  - A: 검색 엔진 인덱싱
  - Q: 매 달 얼마나 수집?
  - A: 10억개
  - Q: 신규 / 수정 페이지 모두 고려?
  - A: Yes
  - Q: 수집된 페이지 저장 필요?
  - A: 5년간 저장 필요
  - Q: 중복 컨텐츠는?
  - A: 무시해도 됨
- 좋은 웹 크롤러가 만족해야 하는 속성
  - 규모 확장성: 병렬처리 잘 활용하면 좋음
  - 안정성 (robustness): 반응 없는 서버, 장애, 악성코드 링크 등을 버텨야 함
  - 예절 (politeness): 수집 대상에 한번에 너무 많이 요청 보내면 안됨
  - 확장성 (extensibility): 신규 형태 컨텐츠 지원하기 쉬워야 함
- 개략적 규모 추정
  - 매달 10억개 다운
  - qps = 10억 / 30일 / 24시간 / 3600 초 = 대략 400페이지/초
  - peak qps = 2 * qps = 800
  - 웹 페이지 평균 크기: 500k
  - 10억 페이지 * 500k = 500TB / 월
  - 5년치 사이즈 = 500TB * 12개월 * 5년 = 30PB

## 2. 개략적 설계안 제시 및 동의 구하기
![그림 9-4](https://user-images.githubusercontent.com/20942871/171606958-676c35de-2c20-4d24-a527-22e6cbc347d5.jpg)

### 시작 URL 집합
- 웹 크롤러가 크롤링을 시작하는 출발점
- 정답은 없으며 필요애 따라 산정
  - 특정 대학의 도메인 이름이 붙은 모든 페이지의 url 을 시작 url 로 사용 가능
  - 나라별 인기있는 사이트로 나눔
  - 스포츠, 쇼핑 등 주제별로 분류

### 미수집 url 저장소
- 대부분의 현대적 웹 크롤러는 크롤링 상태를 다운로드 할 url / 다운로드 된 url 의 2가지로 나눠 관리
- 다운로드할 url 을 저장 관리하는 컴포넌트를 미수집 url 저장소라 부르며 FIFO Queue 로 볼 수 있음


### HTML 다운로더
- 인터넷에서 웹 페이지를 다운받는 컴포넌트

### 도메인 이름 변환기
- URL 을 IP 로 변환

### 컨텐츠 파서
- 웹 페이지 다운 후 parsing / validation 필요
- 크롤링 서버 내 파서를 구현하면 크롤링이 느려질 수 있기에 독립 컴포넌트로 생성

### 중복 컨텐츠 판단
- 연구 결과에 의하면 29% 가량의 웹 페이지 컨텐츠는 중복이기에 같은 데이터를 중복 저장할 수 있음
- 따라서 중복체크하여 비효율적인 부분을 줄여야 함
  - 페이지 내용 전체 비교는 가장 쉬운 방법이지만 가장 비싼 방법임
  - 따라서 대체로 웹 페이지의 해시 값을 비교함

### 컨텐츠 저장소
- HTML 문서를 보관하는 시스템
- 데이터 유형, 크기, 접근빈도, 데이터 유효기간 등을 종합적으로 고려하여 저장소 구현
- 본 예시에선 인기 있는 컨텐츠는 메모리, 그 이외 대부분은 디스크에 저장하도록 구현

### URL 추출기
- HTML 페이지를 파싱하여 안에 있는 링크들을 골라내는 역할
- 내부에 있는 상대 경로는 전부 절대경로로 변경

### URL 필터
- 특정 컨텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록 (deny list) 에 포함된 URL 등을 크롤링 대상에서 배제하는 역할

### 이미 방문한 URL 거르기
- 이미 방문한 적이 있는 URL 을 확인할 수 있으면 같은 URL 을 여러번 추적하는 것을 방지하여 무한루프 막을 수 있음
- bloom filter 나 hash table 을 주로 사용

### URL 저장소
- 이미 방문한 URL 을 보관하는 저장소


## 3. 상세 설계

### DFS? BFS?
- 웹은 방향성이 있는 그래프 (Directed graph) 이며 페이지는 노드, url 은 엣지라고 볼 수 있고, 크롤링 프로세스는 이 엣지를 따라 탐색하는 과정
- DFS 의 경우 예상치 못하게 깊게 들어가버릴 수도 있기에 대부분은 좋지 않음
- 따라서 웹 크롤러는 보통 queue 기반의 BFS 를 사용하는데 여기에도 2가지 문제가 있음
  - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 돌아감: 위키피디아 내 링크들은 대부분 같은 위키피디아 사이트를 가리키므로, 위키피디아 서버에 부담 줄 수 있음
  - 일반적인 BFS 는 url 사이 우선순위를 두지 않음: 품질 높은 사이트와 광고 위주의 쓰레기 사이트를 구별하지 않기에 페이지 링크, 트래픽, 업데이트 빈도 등 어려개를 고려해 우선순위 두는게 좋음

### 미수집 URL 저장소
- 아직 탐색하지 않은 url을 저장하는 저장소로, 이를 잘 구현해야만 예의를 갖춘 크롤러, url 사이의 우선순위와 신선도를 구별하는 구현할 수 있음
- 예의
  - 웹 크롤러는 수집 대상 서버에 짭은 시간 안에 너무 많은 오쳥 보내면 안됨
  - 예의 바른 크롤러 만드는데 있어서 지켜야 할 한가지 원칙: 동일 웹 사이트에 대해서는 한번에 한 페이지만 요청 (같은 웹 페이지를 다운받는 태스크는 시간차 두고 실행)
  - 이를 만족하려면 웹 사이트의 호스터명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지 (매핑 테이블)
  - 각 다운로드 스레드는 별도 fifo 큐를 두고, 해당 큐에서 꺼낸 url 만 다운로드
  - ![그림 9-6](https://user-images.githubusercontent.com/20942871/171610887-f6029374-40a2-4814-8489-ce675a19f1b0.jpg)
- 우선순위
  - 유용성에 따라 URL 우선순위 분배: 페이지 랭크, 트래픽 양, 갱신빈도 등
  - ![그림 9-7](https://user-images.githubusercontent.com/20942871/171611427-fc00fdd2-7def-4dcf-aeb9-703e9e2c6123.jpg)
  - 순위 결정 장치: url 을 입력받아 우선순위 계산
  - 큐: 우선순위별로 큐가 하나씩 할당
  - 큐 선택키: 임의 큐에서 처리할 url 을 꺼내는 역할. 순위가 높을수록 더 자주 꺼내도록 구현
    - ex) 가중치 랜덤 예시: https://blog.naver.com/occidere/222024179048
- 예의 + 우선순위를 모두 확보한 구현
  - 전면 큐: 우선순위 결정
  - 후면 큐: 크롤러의 예의 보증
  - ![그림 9-8](https://user-images.githubusercontent.com/20942871/171611834-ce8238ac-d267-4ba4-a1f5-e4d5deabfd85.jpg)
- 신선도
  - 웹 페이지는 수시로 추가 / 삭제 / 변경
  - 데이터 신선도 유지 위해선 이미 다운한 페이지라도 주기적으로 재수집 필요
  - 그렇다고 다 재수집하면 비싸기에 아래와 같이 최적화 가능
    - 웹 페이지의 변경이력 활용
    - 우선순위 활용해 중요한 페이지는 좀더 자주 재수집
- 저장장치
  - 절충안을 잘 찾아야 함
  - 본 예제에선 일부만 메모리에 두고, 대부분은 디스크에 두는 절충안 택
### HTML 다운로더
- http 프로토콜을 통해 웹 페이지 다운
- Robots.txt
  - 웹 클롤러와 사이트가 소통하는 표준
  - 크롤러가 수집 가능한 목록이 들어있음
  - 크롤링 전 이 파일을 체크해야 함
  - ex) https://www.naver.com/robots.txt
- 성능 최적화
  - 분산 크롤링
    - 크롤링 작업을 여러 서버에 분산하여 성능 향상
  - 도메인 이름 변환 결과 캐시
    - DNS Resolver 는 대표적인 병목 (10ms ~ 200ms)
    - 따라서 도메인 이름과 IP 주소 사이를 매핑해서 캐싱하고 주기적으로 갱신
  - 지역성
    - 크롤링 작업 수행 서버를 지역별로 분산
    - 크롤링 대상 서버와 지역적으로 가까우면 다운 시간 단축
    - 이런 지역성 전략은 크롤 서버, 캐시, 큐 등 대부분에 적용 가능
  - 짧은 타임아웃
    - 어떤 웹 서버는 느리거나 아예 응답하지 않으므로 타임아웃 적절히 선택
- 안정성
  - 시스템 안정성 향상시키기 위한 대표적인 접근법
  - 안정 해시: 다운로더 서버들에 부하를 분산할 때 적용. 다운로드 서버 쉽게 추가/제거 가능
  - 크롤링 상태 및 수집 데이터 저장: 크롤링 상태 기록해 장애 발생한 경우에도 쉽게 복원할 수 있도록
  - 예외 처리: SPOF 방지
  - 데이터 검증
- 확장성
  - 신규 모듈 추가/제거가 쉽게 이뤄지도록 해야 함
- 문제 있는 컨텐츠 감지 및 회피
  - 중복 컨텐츠
    - hash, checksum 등으로 중복 탐지 가능
  - spider trap
    - 크롤러를 무한 루프에 빠뜨리도록 설계된 웹피이지
    - 무한히 깊은 디렉터리 구조가 대표적. ex) sample.com/a/b/a/b/a/b/...
    - url 최대 길이 제한하면 회피 가능
    - 모두 자동 회피는 불가능하기에 블랙리스트 두고 인간이 개입하여 관리하는 등도 필요
  - 데이터 노이즈
    - 광고, 스팸 등 쓰레기 데이터
    - 가치가 없으므로 제외처리 해아 함

## 4. 마무리
- 추가로 논의해보면 좋을 것
  - 서버사이드 렌더링: 자바 스크립트를 사용한 동적 렌더링으로, 그냥 파싱하면 동적 생성 링크는 발견 불가. 동적 크롤링으로 처리
  - 스팸 방지 필트링
  - DB 다중화 및 샤딩
  - 수평적 규모 확장성
  - 가용성, 일관성, 안정성
  - 데이터 분석 솔루션
